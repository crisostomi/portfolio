<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Donato Crisostomi</title>
    <link>//localhost:1313/</link>
      <atom:link href="//localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <description>Donato Crisostomi</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>//localhost:1313/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Donato Crisostomi</title>
      <link>//localhost:1313/</link>
    </image>
    
    <item>
      <title>Topological Signal Processing over Simplicial Complexes (WIP)</title>
      <link>//localhost:1313/project/tsp-sc/</link>
      <pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/tsp-sc/</guid>
      <description>&lt;h1 id=&#34;center-coming-soon-center&#34;&gt;&lt;center&gt; Coming soon! &lt;/center&gt;&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Towards Conditionality for Probabilistic Diffusion Models</title>
      <link>//localhost:1313/project/is/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/is/</guid>
      <description>&lt;!--   We attempt to generate class-conditional images using a probabilistic
  diffusion model by adapting to the latter class-conditional techniques
  initially developed for *GANs*. In particular, we experiment two
  architectures, one leveraging *Conditional Batch Norm* and one
  integrating an *Auxiliary Classifier*, testing different resolutions
  and configurations. The results suggest that these approaches cannot
  be applied to diffusion models as they are: the *CBN* architecture
  results in class-conditional images that do not look realistic while
  the *AC* one yields prettier images but fails to capture the
  conditionality. --&gt;
&lt;hr /&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Richard Feynman once said that&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;What I cannot create, I do not understand.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;and, in the context of machine learning, this means that for machines to understand their input data, they should learn to create it. Moreover, being able to generate unseen images opens the door to some groundshaking applications, from super-resolution, to text-to-image translation (Ledig et al. 2016; Gorti and Ma 2018).&lt;/p&gt;
&lt;p&gt;Image synthesis nevertheless has been one of the most challenging tasks for machine learning to tackle. In fact, generative models are needed to generate new unseen images; but while we witnessed a huge leap forward in discriminative models during the first years of the last decade thanks to neural architectures, generative models initially failed to keep up. It was in &lt;span class=&#34;math inline&#34;&gt;\(2014\)&lt;/span&gt; that &lt;em&gt;Goodfellow et al.&lt;/em&gt; came up with &lt;em&gt;Generative Adversarial Networks&lt;/em&gt; (Goodfellow et al. 2014). GANs and their evolutions have been the state-of-the-art since then, but a very recent paper shows that similar performance can be obtained with a different model that leverages probabilistic diffusion in order to generate images (Ho, Jain, and Abbeel 2020).&lt;/p&gt;
&lt;p&gt;Generative models have also been particularly useful to create artificial examples in order to augment datasets (Santos Tanaka and Aranha 2019), but in order to generate new images belonging to a certain class, one would need to have a conditional model. Goal of this research is therefore to integrate class-conditionality in probabilistic diffusion models.&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related work&lt;/h1&gt;
&lt;p&gt;As anticipated, the most used generative architecture is &lt;em&gt;Generative Adversarial Networks&lt;/em&gt; (Goodfellow et al. 2014), which is composed of two networks that are trained &lt;em&gt;adversarily&lt;/em&gt;: a &lt;em&gt;generator&lt;/em&gt; is trained in such a way that a &lt;em&gt;discriminator&lt;/em&gt; cannot distinguish between its generated samples and the real ones, while simultaneously training the discriminator to be able to distinguish between fake samples and real ones. To integrate class-conditionality in GANs, various approaches have been tried: &lt;em&gt;Brock et al.&lt;/em&gt; provide class information to the generator with &lt;em&gt;conditional batch norm&lt;/em&gt; (Brock, Donahue, and Simonyan 2018), while &lt;em&gt;Odena et al.&lt;/em&gt; leverage an auxiliary classifier (Odena, Olah, and Shlens 2017).&lt;/p&gt;
&lt;h1 id=&#34;proposed-method&#34;&gt;Proposed method&lt;/h1&gt;
&lt;p&gt;A &lt;em&gt;diffusion probabilistic model&lt;/em&gt; is a parameterized &lt;em&gt;Markov chain&lt;/em&gt;: A Markov chain models the state of a system with a random variable that changes through time. For the Markov property to hold, the distribution of a state must depend only on the distribution of the previous state.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/markov-diffusion.png&#34; id=&#34;fig:markov&#34; alt=&#34;The directed graphical model used in the project.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;The directed graphical model used in the project.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The training phase consists of two phases: a forward pass and a reverse pass, as can be seen in &lt;a href=&#34;#fig:markov&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:markov&#34;&gt;1&lt;/a&gt;. In the former, also called the diffusion process, Gaussian noise is added to the image according to a fixed schedule so each transition in the Markov chain &lt;span class=&#34;math inline&#34;&gt;\(q(\mathbf{x}_{t}| \mathbf{x}_{t-1})\)&lt;/span&gt; represents the addition of Gaussian noise. In the latter, the transitions of a reverse Markov chain are learned in order to reconstruct the destroyed signal; the parameters are learned by optimizing the variational bound on negative loglikelihood:&lt;/p&gt;
&lt;div class=&#34;math&#34;&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \mathbb{E}\left[ - \log p_{\theta}(\mathbf{x}_0) \right] &amp;amp;\leq \mathbb{E}_q \left[ - \log \frac{p_\theta (\mathbf{x}_0, \dots, \mathbf{x}_T)}{q(\mathbf{x}_1, \dots, \mathbf{x}_T|\mathbf{x}_0)}\right] \\
  &amp;amp;= \mathbb{E}_q\left[ - \log \ p(\mathbf{x}_T) - \sum_{t \geq 1} \log \frac{p_\theta (\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t | \mathbf{x}_{t-1})} \right]
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We employ the architecture suggested by the original paper, in which the denoiser is a &lt;em&gt;U-Net&lt;/em&gt; (Ronneberger, Fischer, and Brox 2015), shown in &lt;a href=&#34;#fig:u-net&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:u-net&#34;&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/U-net.png&#34; id=&#34;fig:u-net&#34; alt=&#34;The popular U-Net architecture used for the denoiser.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;The popular &lt;em&gt;U-Net&lt;/em&gt; architecture used for the denoiser.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;conditional-batch-norm&#34;&gt;Conditional Batch Norm&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Conditional Batch Normalization&lt;/em&gt; was first applied to language-vision tasks to implement the intuition that the linguistic input should modulate the entire visual processing, instead of being fused only in the last part of the process. &lt;em&gt;CBN&lt;/em&gt; builds upon &lt;em&gt;Batch Normalization&lt;/em&gt;, in which each batch is normalized as follows to reduce the internal co-variate shift &lt;span class=&#34;math display&#34;&gt;\[\text{BN}_{\gamma, \beta}(x_i) = \gamma_i \frac{x_i - \mathbb{E}(x_i)}{\sqrt{var(x_i)}} + \beta_i\]&lt;/span&gt; In &lt;em&gt;CBN&lt;/em&gt; we want to predict &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; from an embedding of the class, so that the class may manipulate entire feature maps by scaling them up or down, negating them, or shutting them off completely (Odena, Olah, and Shlens 2017).&lt;/p&gt;
&lt;p&gt;The integration of &lt;em&gt;CBN&lt;/em&gt; in the architecture is done by replacing the &lt;em&gt;Batch Norm&lt;/em&gt; layers inside the denoiser architecture with &lt;em&gt;conditional&lt;/em&gt; ones. We are going to refer to the model obtained by adding &lt;em&gt;CBN&lt;/em&gt; to the original model as &lt;span class=&#34;math inline&#34;&gt;\(M_{CBN}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;auxiliary-classifier&#34;&gt;Auxiliary Classifier&lt;/h2&gt;
&lt;p&gt;Analogously to what has been done in (Odena, Olah, and Shlens 2017) for GANs, we have added an auxiliary classifier to the original architecture of the denoiser.&lt;/p&gt;
&lt;p&gt;To provide the class information to the denoiser, the label is embedded and reshaped to be the same dimension as one of the channels of the image, &lt;em&gt;i.e.&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(w \times h\)&lt;/span&gt;, and then concatenated to the input image in the channel dimension. Images are thus tensors of shape &lt;span class=&#34;math inline&#34;&gt;\((b, c+1, w, h)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the batch size, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the number of channels (RGB), &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is the width and &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is the height.&lt;/p&gt;
&lt;p&gt;The overall loss is then obtained as a weighted sum of the variational loss to account for the reconstruction error, and the classifier loss, which is a categorical cross entropy, where the weight is a hyper-parameter. The loss should this way be enriched with class information that should backpropagate to the parameters that are involved in the generation.&lt;/p&gt;
&lt;p&gt;We are going to refer to the model obtained by adding the auxiliary classifier to the original model as &lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;
&lt;p&gt;We based our implementation on the following repository &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;denoising-diffusion-pytorch&lt;/a&gt;, which provides a working PyTorch baseline.&lt;/p&gt;
&lt;p&gt;Our original goal was to apply the model to the &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_IP102_A_Large-Scale_Benchmark_Dataset_for_Insect_Pest_Recognition_CVPR_2019_paper.pdf&#34;&gt;insect-pest dataset&lt;/a&gt; to create new artificial samples for dataset augmentation. The original dataset consisted of over &lt;span class=&#34;math inline&#34;&gt;\(75k\)&lt;/span&gt; images, but most of the classes had few samples and low variance between them, we therefore used a subsample of &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; classes, ammounting to &lt;span class=&#34;math inline&#34;&gt;\(\approx 25\)&lt;/span&gt;k samples. The dataset is not really what a data scientist would dream of, as no bounding boxes were provided, and it is often hard even for humans to understand what’s in the image. To attribute the right degree of responsibility to the model and to the dataset, we also tested the model on a different dataset from Stanford, containing &lt;span class=&#34;math inline&#34;&gt;\(\approx 20k\)&lt;/span&gt; images of cars.&lt;/p&gt;
&lt;p&gt;To test our proposed conditional methods, to simplify the visual inspection of the results, we instead created an ad-hoc dataset of only two classes with the aim of maximizing the difference between them. To this end, we took a subset of &lt;span class=&#34;math inline&#34;&gt;\(\approx 10k\)&lt;/span&gt; images from the &lt;em&gt;Stanford dogs&lt;/em&gt; (Khosla et al. 2011) and the &lt;em&gt;Stanford cars&lt;/em&gt; (Krause et al. 2013) datasets.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;All the unconditional and conditional versions of the model that follow have been trained for &lt;span class=&#34;math inline&#34;&gt;\(\approx 100\)&lt;/span&gt; epochs. The unconditional model was tested both on low resolution sample and higher resolution ones, yielding the results that follow.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;64&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;128&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/unconditional-insects-64.png&#34; id=&#34;fig:insects-64&#34; alt=&#34;Unconditional model applied to the insect-pest dataset with resolution 64.&#34; /&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/unconditional-insects-128.png&#34; id=&#34;fig:insects-128&#34; alt=&#34;Unconditional model applied to the insect-pest dataset with resolution 128.&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As it is evident from the samples, the resolution plays a strong role in generating realistic images, providing the model more information to leverage for the generation. The Inception Scores are as follow&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;64&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;128&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;insects&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;4.2&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;4.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;cars&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;3.32&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;Regarding the conditional model, the two proposed methods yielded totally different results. &lt;span class=&#34;math inline&#34;&gt;\(M_{CBN}\)&lt;/span&gt; converges to a small reconstruction error, and the class of the generated images can often be inferred visually; see for example &lt;a href=&#34;#fig:cbn-generated-dogs&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:cbn-generated-dogs&#34;&gt;6&lt;/a&gt; which is a batch of generated images for the ‘dog’ class and &lt;a href=&#34;#fig:cbn-generated-cars&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:cbn-generated-cars&#34;&gt;7&lt;/a&gt; which is a batch of generated images for the ‘car’ class.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;class ‘dog’&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;class ‘car’&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/cbn-generated-dogs.png&#34; id=&#34;fig:cbn-generated-dogs&#34; alt=&#34;Images generated by M_{CBN} when supplied class ‘dog.’&#34; /&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/cbn-generated-cars.png&#34; id=&#34;fig:cbn-generated-cars&#34; alt=&#34;Images generated by M_{CBN} when supplied class ‘car.’&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Nevertheless, the results appear as messy color spots which do not resemble any realistic image. As the reconstruction error is small, the problem seems to be related to the sampling procedure, and indeed it might be the case that the class information is not accounted for correctly during sampling, as the &lt;em&gt;CBN&lt;/em&gt; is only part of the denoiser and class information does not influence the rest of the sampling process.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/CBN-t-sne-points.png&#34; id=&#34;fig:cbn-t-sne-points&#34; alt=&#34;t-sne plot of the images generated by M_{CBN}.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;&lt;em&gt;t-sne&lt;/em&gt; plot of the images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{CBN}\)&lt;/span&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To check whether there is a class-related distinction between the generated images, we plotted the images with &lt;em&gt;t-SNE&lt;/em&gt;, yielding the results that can be seen in &lt;a href=&#34;#fig:cbn-t-sne-points&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:cbn-t-sne-points&#34;&gt;11&lt;/a&gt; and &lt;a href=&#34;#fig:cbn-t-sne-images&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:cbn-t-sne-images&#34;&gt;12&lt;/a&gt;. The points seem to be fairly separable, indicating that the class is indeed infused in the generated images.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/CBN-t-sne-images.png&#34; id=&#34;fig:cbn-t-sne-images&#34; alt=&#34;t-sne plot of the images generated by M_{CBN} with each point visualized as the image that it embeds.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;&lt;em&gt;t-sne&lt;/em&gt; plot of the images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{CBN}\)&lt;/span&gt; with each point visualized as the image that it embeds.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt; instead results in the converse, yielding almost realistic images that do not seem to be much influenced by the class. As a first attempt, we tried training the random-initialized classifier with the rest of the architecture; this resulted in a rapidly decreasing classifier loss that did not help the generation at all, but instead seemed to only worsen the results. To our advise, this was due to a process of co-adaption in which the parameters of one computational block were set to satisfy the other, and viceversa. To address this issue, we pretrained the classifier until convergence on the dataset of real images and then kept its parameters fixed during the training of the rest of the model. This yielded the results in &lt;a href=&#34;#fig:ac-generated-dogs&#34;&gt;8&lt;/a&gt; and &lt;a href=&#34;#fig:ac-generated-cars&#34;&gt;9&lt;/a&gt;;&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;class ‘dog’&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;class ‘car’&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/ac-generated-dogs.png&#34; id=&#34;fig:ac-generated-dogs&#34; alt=&#34;Images generated by M_{AC} when supplied class ‘dog.’&#34; /&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/ac-generated-cars.png&#34; id=&#34;fig:ac-generated-cars&#34; alt=&#34;Images generated by M_{AC} when supplied class ‘car.’&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The images resemble cars, mostly ignoring the input label. We eventually tried feeding higher-resolution images to the model, but with no significant improvement. The generated images in fact do not resemble their class, but the classification loss still goes rapidly down; to provide an explanation, we visually inspected the images and found out that artifacts were present in every image (&lt;a href=&#34;#fig:ac-generated-128&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:ac-generated-128&#34;&gt;10&lt;/a&gt;), probably resulting from the generator ‘tricking’ the classifier, emphasizing features that resulted in high confidence guesses in the latter.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/ac-generated-128.png&#34; id=&#34;fig:ac-generated-128&#34; alt=&#34;Images generated by M_{AC} when supplied class ‘car’ at resolution 128; artifacts are circled.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt; when supplied class ‘car’ at resolution &lt;span class=&#34;math inline&#34;&gt;\(128\)&lt;/span&gt;; artifacts are circled.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We concluded that the model was not robust enough, and therefore tried to employ a finetuned &lt;em&gt;ResNet18&lt;/em&gt; classifier, but this did not solve the issue.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/AC-t-sne-points.png&#34; id=&#34;fig:ac-t-sne-points&#34; alt=&#34;t-sne plot of the images generated by M_{AC}.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;&lt;em&gt;t-sne&lt;/em&gt; plot of the images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As before, we plotted the images with &lt;em&gt;t-SNE&lt;/em&gt; to check whether there is a class-related distinction between the images; as can be seen in &lt;a href=&#34;#fig:ac-t-sne-points&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:ac-t-sne-points&#34;&gt;13&lt;/a&gt; and &lt;a href=&#34;#fig:ac-t-sne-images&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:ac-t-sne-images&#34;&gt;14&lt;/a&gt;, this time the points are all mixed up, indicating that the model fails to conditionate the generation on the class.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/AC-t-sne-images.png&#34; id=&#34;fig:ac-t-sne-images&#34; alt=&#34;t-sne plot of the images generated by M_{AC} with each point visualized as the image that it embeds.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;&lt;em&gt;t-sne&lt;/em&gt; plot of the images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt; with each point visualized as the image that it embeds.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;The proposed methods do not yield acceptable results, indicating that it is not enough to adapt GANs techniques for class-conditionality to probabilistic diffusion models, while this is also not straightforward to do. This also emphasizes that, while seemingly close to GANs, this family of models requires ad-hoc research, as they are based on different theorical aspects.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; role=&#34;doc-bibliography&#34;&gt;
&lt;div id=&#34;ref-Brock2018gan&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Brock, Andrew, Jeff Donahue, and Karen Simonyan. 2018. “Large Scale GAN Training for High Fidelity Natural Image Synthesis.” &lt;em&gt;CoRR&lt;/em&gt; abs/1809.11096. &lt;a href=&#34;http://arxiv.org/abs/1809.11096&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1809.11096&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-goodfellow2014generative&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Networks.” &lt;a href=&#34;http://arxiv.org/abs/1406.2661&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1406.2661&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-text2image&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Gorti, Satya Krishna, and Jeremy Ma. 2018. “Text-to-Image-to-Text Translation Using Cycle Consistent Adversarial Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1808.04538. &lt;a href=&#34;http://arxiv.org/abs/1808.04538&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1808.04538&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-ho2020denoising&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. “Denoising Diffusion Probabilistic Models.” &lt;a href=&#34;http://arxiv.org/abs/2006.11239&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/2006.11239&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-stanford-dogs&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Khosla, Aditya, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. 2011. “Novel Dataset for Fine-Grained Image Categorization.” In &lt;em&gt;First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. Colorado Springs, CO.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-stanford-cars&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Krause, Jonathan, Michael Stark, Jia Deng, and Li Fei-Fei. 2013. “3d Object Representations for Fine-Grained Categorization.” In &lt;em&gt;4th International IEEE Workshop on 3d Representation and Recognition (3dRR-13)&lt;/em&gt;. Sydney, Australia.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-super-resolution&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Ledig, Christian, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew P. Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. 2016. “Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.” &lt;em&gt;CoRR&lt;/em&gt; abs/1609.04802. &lt;a href=&#34;http://arxiv.org/abs/1609.04802&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1609.04802&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-odena2017conditional&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Odena, Augustus, Christopher Olah, and Jonathon Shlens. 2017. “Conditional Image Synthesis with Auxiliary Classifier GANs.” &lt;a href=&#34;http://arxiv.org/abs/1610.09585&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1610.09585&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-U-net&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” &lt;em&gt;CoRR&lt;/em&gt; abs/1505.04597. &lt;a href=&#34;http://arxiv.org/abs/1505.04597&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1505.04597&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-data-augmentation&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Santos Tanaka, Fabio Henrique Kiyoiti dos, and Claus Aranha. 2019. “Data Augmentation Using GANs.” &lt;em&gt;CoRR&lt;/em&gt; abs/1904.09135. &lt;a href=&#34;http://arxiv.org/abs/1904.09135&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1904.09135&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Virality Prediction via Graph Neural Networks</title>
      <link>//localhost:1313/project/vp/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/vp/</guid>
      <description>&lt;hr /&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;What is virality? Virality, in its original meaning, refers to viruses that can only survive by continously spreading from one host to another in a parasitic manner; Actually, many real life phenomena exhibit a &lt;em&gt;spreading behaviour&lt;/em&gt; to which we can extend the notion of virality.&lt;/p&gt;
&lt;p&gt;The ability to predict the spreading potential of a certain signal has evident benefits, for example providing a mean to prevent the spread of undesired phenomena such as diseases or fake news, but also allowing companies to exploit this information to improve their advertising campaigns.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Graphs&lt;/em&gt; serve as an useful abstraction to model real world situations, and are well suited to represent spreading patterns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;nodes&lt;/em&gt; represent components of interest (e.g. users in a social network);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;edges&lt;/em&gt; define existing relations among these components;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;node signal&lt;/em&gt; represents the information, which is generated from some source node, and is propagated to its neighboring nodes through its edges, possibly iterating the process until all the nodes have been reached.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;task-formalization&#34;&gt;Task formalization&lt;/h2&gt;
&lt;p&gt;A spreading piece of information &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; originates a &lt;em&gt;cascade&lt;/em&gt; in the network, to formalize the problem as a learning task we distinguish two sets, namely&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;early adopters&lt;/em&gt;, and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;final adopters&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The former are the ones producing the information as they don’t receive it from other nodes, while the latter are those who adopt the information at the end of the propagation process, or, if you think about it as a disease, those who get infected.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/T2.png&#34; id=&#34;fig:spread-proc&#34; alt=&#34;Spreading process.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Spreading process.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In the example, the information is originally produced by nodes &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; independently, and is then spread in subsequent moments until it stops. The final adopters will be all the nodes who have been reached by the information, including the early adopters.&lt;/p&gt;
&lt;p&gt;So, after a preprocessing step, each node will be characterized by the following two features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;whether it is an early adopter: &lt;span class=&#34;math display&#34;&gt;\[s_{v}^{(0)} = \text{initial activation state of node } v\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;and whether it is a final adopter, which is the label we want to predict: &lt;span class=&#34;math display&#34;&gt;\[s_{v}^{(T)} = \text{final activation state of node } v\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final virality coefficient for the piece of information &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is eventually obtained by counting the final adopters. &lt;span class=&#34;math display&#34;&gt;\[\mathcal{P}_{m} = \sum_{v \in \mathcal{V}} s_{v}^{(T)} = n_{\infty}^m\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;approaches&#34;&gt;Approaches&lt;/h2&gt;
&lt;p&gt;As a node prediction task, both &lt;em&gt;feature-based&lt;/em&gt; methods and &lt;em&gt;representation learning&lt;/em&gt; methods can be exploited. The former approach heavily depends on the quality of the hand-crafted features, which are generally extracted heuristically, while the latter allows to automatically learn representations of node statuses which are suited for the task at hand. A possible way to do this is by embedding the graphs into a vector space, and then using conventional representation learning techniques; nevertheless, a more natural approach would be to instead generalize the machine learning models to non-euclidean domains: in the case of deep learning models, this is usually called &lt;em&gt;geometric deep learning&lt;/em&gt;.&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;For our task, both synthetic data and real world data have been used.&lt;/p&gt;
&lt;h2 id=&#34;synthetic-data&#34;&gt;Synthetic data&lt;/h2&gt;
&lt;p&gt;The synthetic data generation involves two steps:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;generating the social structure of interest;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;generating a certain number of information cascades;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;social-structure&#34;&gt;Social structure&lt;/h3&gt;
&lt;p&gt;To artificially generate a social network structure which resembles a real one, &lt;em&gt;random graph models&lt;/em&gt; are usually used. A good model should allow creating graphs for which the degree distribution follows a power-law, as happens in real social networks.&lt;/p&gt;
&lt;p&gt;A power law is a functional relationship &lt;span class=&#34;math inline&#34;&gt;\(y = ax^{-c}\)&lt;/span&gt; between two quantities, where one quantity varies as a power of the other.&lt;/p&gt;
&lt;p&gt;By applying the logarithm to both parts we have that &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
    y &amp;amp;= ax^{-c} \\
    log(y) &amp;amp;= log(ax^{-c}) \\
    log(y) &amp;amp;= log(a) -c \cdot log(x)\end{aligned}\]&lt;/span&gt; As a consequence, we get that a power law appears as a line in a log log scale plot, as can be seen in the Twitter degree distribution in figure.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/degree-distr.png&#34; id=&#34;fig:twit-deg-distr&#34; alt=&#34;Twitter degree distribution.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Twitter degree distribution.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In the social network context it means that it is exponentially more likely to pick “normal people” with few friends or followers rather than popular profiles, called “celebrities” or “authorities”.&lt;/p&gt;
&lt;p&gt;For this reason we opted for a &lt;em&gt;preferential attachment&lt;/em&gt; model, which works in the following way: you begin with a single node with a self loop, when you have built a graph with &lt;span class=&#34;math inline&#34;&gt;\(N-1\)&lt;/span&gt; nodes, you add the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;-th node with an edge that goes from &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; to a node &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; chosen accordingly with a probability proportional to the degree of &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Inductive definition of the model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Base step: &lt;span class=&#34;math inline&#34;&gt;\(G_1\)&lt;/span&gt; is a single node with a self loop;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Inductive step (for &lt;span class=&#34;math inline&#34;&gt;\(i = 2, 3, \ldots\)&lt;/span&gt;):&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;add node &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(G_i\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;add a “&lt;em&gt;half edge&lt;/em&gt;” coming out from node &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;choose a node &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; randomly with probability proportional to its degree, i.e., &lt;span class=&#34;math inline&#34;&gt;\(P\left\{\text{neighbor of $N$ is $i$}\right\} = \frac{deg(i)}{\sum_{k=1}^{N} deg(k)}\)&lt;/span&gt;, where the denominator is a normalization factor;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;close the &lt;em&gt;half edge&lt;/em&gt; from &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, by connecting it to &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;information-cascades&#34;&gt;Information cascades&lt;/h3&gt;
&lt;p&gt;The cascades are generated with the &lt;em&gt;Independent Cascades&lt;/em&gt; model, which works in the following way: Let’s assume we have &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; nodes holding some piece of information (the seed set), the time is discrete and this information spreads over time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;at time &lt;span class=&#34;math inline&#34;&gt;\(t_0\)&lt;/span&gt; the only persons having the information will be the ones in the seed seet;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;at time &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; for each of the edges incident on the nodes having the information we will be flipping a coin:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;with prob &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; the information will spread on that edge;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;else the edge is lost forever.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;real-data&#34;&gt;Real data&lt;/h2&gt;
&lt;p&gt;Similarly to the synthetic data generation, the process to obtain real data from Twitter involved two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;retrieving the social network relative to a subgraph of Twitter;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;obtaining the cascades from the tweets of the users in the subgraph.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;social-structure-1&#34;&gt;Social structure&lt;/h3&gt;
&lt;p&gt;To obtain a subgraph of Twitter we scraped the social network in a &lt;em&gt;Breadth First&lt;/em&gt;-fashion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;start with a queue containing a random english speaking user;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;collect all his followers and followees and add them to the queue;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;pop the next user from the queue and repeat step 2 until the desired number of users is reached;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cascades&#34;&gt;Cascades&lt;/h3&gt;
&lt;p&gt;Given the set of users &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; collected in the previous step, we obtained all the tweets published by users in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; that fell in a certain time-window.&lt;/p&gt;
&lt;p&gt;So, obtained the hashtags from the set of tweets, we recreate for each distinct hashtag a propagation cascade in the following way:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;order the tweets containing the hashtags by timestamp;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;create the first cascade with the first tweet author as root node;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for each remaining tweet &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; be the node relative to the author of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; has an incoming edge from an existing cascade tree &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, then add it to &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;else create a new cascade tree with &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; as root;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The roots of the cascade trees were used as early adopters, the remaining nodes as final.&lt;/p&gt;
&lt;p&gt;The scraping process resulted in a dataset containing &lt;span class=&#34;math inline&#34;&gt;\(~30k\)&lt;/span&gt; users connected by &lt;span class=&#34;math inline&#34;&gt;\(~400k\)&lt;/span&gt; edges, which published a total of &lt;span class=&#34;math inline&#34;&gt;\(12912921\)&lt;/span&gt; tweets. Among these, [..] contained hashtags, if an hashtag was posted more than once from the same user in the given time window it was considered only once.&lt;/p&gt;
&lt;h3 id=&#34;sparsity&#34;&gt;Sparsity&lt;/h3&gt;
&lt;p&gt;The collected dataset, as you can see in the first plot, suffers from severe sparsity; Most of the hashtags appear in tweets of just one or two distinct authors.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/hashtags_distinct.png&#34; alt=&#34;image&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;image&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Even worse, also ignoring hashtags which have been tweeted only by one author, most of the cascades are shallow.&lt;/p&gt;
&lt;p&gt;In the piechart, we see that among all the cascades the great majority of them is just made of a single node, meaning that in most cases there is no spreading tree structure at all, but rather a set of indipendent nodes who hold the same information.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/cascades_pie.png&#34; alt=&#34;image&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;image&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This is due to two reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;first, virality is intrinsecally rare: this may result surprising to us because we can come up with many viral examples, but this is a biased sampling because all the contents which are not viral don’t come up to our minds because we never see them at all; If we take the ratio of viral contents over all the contents we would in fact see that they are a great minority;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;second, we are observing a small subnetwork of the real social network; this way, cascades that would be deep in the real network may instead appear to us a set of independent shallow cascades, as the subgraph is by construction incomplete and may therefore miss the nodes which keep the subcascades connected in the real network;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;node-features&#34;&gt;Node features&lt;/h2&gt;
&lt;p&gt;The representation learning techniques may fail to capture some local node properties, for this reason these can be preprocessed and used to enrich the nodes before passing them as input to the model;&lt;/p&gt;
&lt;p&gt;For each node, we computed the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;local clustering coefficient&lt;/em&gt;, which quantifies how close its neighbours are to being a clique; &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
        C_{i} &amp;amp;= \frac{\text{# of existing edges in $N(v_i)$} }{\text{# of all possible edges in $N(v_i)$}} 
    \end{aligned}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(N(v_i)\)&lt;/span&gt; is the neighborhood of &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_i\)&lt;/span&gt; is the number of neighbors &lt;span class=&#34;math inline&#34;&gt;\(|N(v_i)|\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;eigenvector centrality&lt;/em&gt;, which measures the node influence in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;PageRank&lt;/em&gt; coefficient, which is a kind of eigenvector centrality which was originally used by Google to represent the likelihood that a person randomly clicking on links will arrive at any particular webpage;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Authority&lt;/em&gt; and &lt;em&gt;Hubs&lt;/em&gt; coefficients, the intuition here is that a good hub represents a node that points to many other node, while a good authority represents a node that is linked by many different hubs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;model&#34;&gt;Model&lt;/h1&gt;
&lt;h2 id=&#34;generalizing-convolution&#34;&gt;Generalizing convolution&lt;/h2&gt;
&lt;p&gt;Graphs are &lt;em&gt;non-Euclidean&lt;/em&gt; domains, meaning that they do not share the flat, grid-like structure of the &lt;em&gt;Euclidean&lt;/em&gt; space, but instead have a non-trivial structure; this structure is informative, and should be accounted for along with the information coming from the data on the domain. Nevertheless, many of the operations employed by the building blocks of deep neural networks rely on this structure, convolution being one of them. The latter enforces by construction useful priors that we would like to inject in our learning models, like &lt;em&gt;self-similarity&lt;/em&gt; and &lt;em&gt;locality&lt;/em&gt;, that have their importance also in the graph setting. Nonetheless, convolution cannot naturally be applied to &lt;em&gt;non-Euclidean&lt;/em&gt; domains, and so different approaches have been suggested over the last years. For this project, we have employed two architectures which exploit totally different theoretical frameworks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Graph Attention Networks&lt;/em&gt;, which fall under the category of spatial approaches;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Graph Convolutional Networks&lt;/em&gt;, which instead leverage spectral theory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;graph-convolutional-network&#34;&gt;Graph Convolutional Network&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Spectral&lt;/em&gt; approaches have this name since they define the convolution operation on graphs’ nodes in the &lt;em&gt;spectral&lt;/em&gt;, or &lt;em&gt;Fourier&lt;/em&gt;, domain as the multiplication of a node signal &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x} \in \mathbb{R}^n\)&lt;/span&gt; with a filter &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{g}_{\theta} = diag(g_{\theta}^{(1)}, \dots, g_{\theta}^{(n)})\)&lt;/span&gt; in the Fourier domain.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{g}_{\theta} \star \mathbf{x} = \mathbf{U} \mathbf{g}_{\theta} \mathbf{U}^{\top} \mathbf{x}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This definition exploits several properties. The first is the &lt;strong&gt;convolution theorem&lt;/strong&gt;. The convolution theorem is a defining property of convolution and states that the Fourier transform diagonalizes convolution. &lt;span class=&#34;math display&#34;&gt;\[\mathcal{F}\{ (\mathbf{g} \star \mathbf{x}) \} = \underbrace{\mathcal{F}\{ \mathbf{g} \} \mathcal{F}\{ \mathbf{x} \} }_{\text{simple product}}\]&lt;/span&gt; This means that the convolution of two signals, that in our case would be a node signal &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^n\)&lt;/span&gt; and a parametrized filter &lt;span class=&#34;math inline&#34;&gt;\(g_\theta,\)&lt;/span&gt; is a simple product, in the &lt;em&gt;Fourier&lt;/em&gt; domain. However, the &lt;em&gt;Fourier transform&lt;/em&gt; of a signal requires an integral, so it is not clearly defined on &lt;em&gt;non-Euclidean&lt;/em&gt; domains, and so far we have only shifted the problem from convolution to &lt;em&gt;Fourier transform&lt;/em&gt;. On the other hand, there is an operator, the &lt;em&gt;Laplacian&lt;/em&gt;, that is a differential operator in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^n\)&lt;/span&gt; but that can be easily generalized to &lt;em&gt;non-Euclidean domains&lt;/em&gt;, and for instance here we see its graph counterpart &lt;span class=&#34;math display&#34;&gt;\[\Delta \mathbf{f} = \underbrace{\left( \mathbf{I}_n - \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)}_{\text{normalized graph Laplacian}} \mathbf{f}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What is the connection between the two? We can think of the Fourier transform of a function as expressing that function as a weighted average of functions, with some proper coefficients. Looking at the formula, the coefficients are the values taken by the original function, while the the functions are members of the so called &lt;em&gt;Fourier basis&lt;/em&gt;, and in the case of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^n\)&lt;/span&gt; are called plane waves, since they are complex sinusoids. &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
    \mathcal{F}\{ f(x) \} = \hat{f}(x) = \int f(x) \overbrace{e^{-2\pi i x \xi }}^{\text{plane waves are Fourier basis}} dx \\
    \Delta \underbrace{\left( e^{-2\pi i x \xi} \right)}_{\text{plane wave}} = 4 \pi^2 |\xi|^2 \underbrace{e^{-2\pi i x \xi}}_{\text{Laplacian eigenfunction}}\end{aligned}\]&lt;/span&gt; It turns out that these plane waves are eigenfunctions of the &lt;em&gt;Laplacian&lt;/em&gt;. We can now exploit this property by defining the &lt;em&gt;Fourier basis&lt;/em&gt; on graphs to be the eigenvectors of the graph &lt;em&gt;Laplacian&lt;/em&gt;, so that performing a &lt;em&gt;Fourier transform&lt;/em&gt; is as simple as multiplying by the transposed matrix of eigenvectors. &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
    \Delta = \mathbf{U} \mathbf*{\Lambda} \mathbf{U}^{\top} \\
    \mathbf{\hat{x}} = \mathbf{U}^{\top} \mathbf{x}, \qquad \mathbf{x} = \mathbf{U} \mathbf{\hat{x}}\end{aligned}\]&lt;/span&gt; Now, the initial formula is explained as bringing the node signal &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; in the &lt;em&gt;Fourier&lt;/em&gt; domain, performing convolution as a simple element-wise, product, and then go back to the spatial domain. &lt;span class=&#34;math display&#34;&gt;\[\mathbf{g}_{\theta} \star \mathbf{x} = \underbrace{\mathbf{U}}_{\text{back to spatial domain}} \overbrace{\mathbf{g}_{\theta}}^{\text{conv. in Fourier domain}} \underbrace{\mathbf{U}^{\top} \mathbf{x}}_{\text{to Fourier domain}}\]&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{g}_{\theta} = \mathbf{g}_{\theta}(\mathbf*{\Lambda}) =\)&lt;/span&gt; learnable &lt;em&gt;spectral kernel functions&lt;/em&gt; of the &lt;em&gt;Laplacian&lt;/em&gt; eigenvalues.&lt;/p&gt;
&lt;p&gt;Now, this was the theoretical background to define spectral convolution. Then, different spectral approaches implement this operation in different ways. For instance, the operation that the &lt;em&gt;GCN&lt;/em&gt; layer implements is a simplification. In particular, two main simplifications are made. The first is that computing &lt;span class=&#34;math inline&#34;&gt;\(g_\theta\)&lt;/span&gt;, as a function of the eigenvalues, requires an eigendecomposition which is computationally expensive. So, we can approximate it as a truncated expansion in terms of &lt;em&gt;Chebyshev&lt;/em&gt; polynomials. These polynomials form an orthogonal basis for functions defined on the unit circle, so if we properly renormalize the matrix of eigenvalues we can approximate the filter up to some precision &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. This means that convolution now has the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
    \mathbf{g}_{\theta}(\mathbf*{\Lambda}) \approx \sum_{k=0}^K \theta_k&amp;#39; T_k \underbrace{(\mathbf*{\tilde{\Lambda}})}_{\text{renormalized}} \\
    \mathbf{g}_{\theta}&amp;#39; \star \mathbf{x} \approx \sum_{k=0}^K \theta_k&amp;#39; T_k (\mathbf*{\tilde{L}}) \mathbf{x}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice how the Laplacian enters up to its &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-th power, meaning that the output of the convolution for each node will depend on node signals from their &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-th order neighborhood. The second simplification is that there is no reason to aggregate a &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-order neighborhood, instead we could just stack &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; layers, each computing one hop. By restricting &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; to 1 we get &lt;span class=&#34;math display&#34;&gt;\[\mathbf{g}_{\theta}&amp;#39; \star \mathbf{x} \approx \theta_0&amp;#39; \mathbf{x} - \theta_1&amp;#39; \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}} \mathbf{x}\]&lt;/span&gt; The final simplification is just to reduce the number of free parameters, so we arrive to the actual implementation of the &lt;em&gt;GCN&lt;/em&gt; layer. &lt;span class=&#34;math display&#34;&gt;\[\mathbf{g}_{\theta}&amp;#39; \star \mathbf{x} \approx \overbrace{\theta}^{\text{learnable}} \underbrace{\left( \mathbf{I}_n +  \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)}_{\text{fixed}} \mathbf{x}\]&lt;/span&gt; Notice that this whole expression is fixed, meaning it has no learnable parameters and is computed once as a preprocessing step.&lt;/p&gt;
&lt;h2 id=&#34;graph-attention-network&#34;&gt;Graph Attention Network&lt;/h2&gt;
&lt;p&gt;Now, &lt;em&gt;GCN&lt;/em&gt; suffers from several problems. The first is something inherent to all spectral approaches, they cannot be transferred to unseen graphs. In particular, for &lt;em&gt;G&lt;/em&gt;CN since the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf*{\tilde{A}} = \mathbf{I}_n + \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}\)&lt;/span&gt; is computed with degree and adjacency matrix of the training graph, that will be different for unseen graphs. This is not limiting for us, since the social graph is indeed fixed, but can of course be very limiting.&lt;/p&gt;
&lt;p&gt;The second is that the learnable parameters &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}\)&lt;/span&gt; are shared across the nodes in a neighborhood. Here, the neighborhood of node 1, that has signal &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;, is associated with &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;, meaning all the nodes in this neighborhood have importance &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;. As said before, &lt;em&gt;GAT&lt;/em&gt; is a type of spatial approach, that addresses these problems by defining convolution directly in the spatial domain. In particular, a convolutional attention layer does the following computation. It receives in input a set of node features. &lt;span class=&#34;math display&#34;&gt;\[\mathbf{H} = \{\ \mathbf{h}_1, \dots, \mathbf{h}_n \}, ~ \mathbf{h}_i \in \mathbb{R}^F\]&lt;/span&gt; Then applies a shared linear transformation to every node. &lt;span class=&#34;math display&#34;&gt;\[\mathbf{h}_i \mapsto \mathbf{W} \mathbf{h}_i = \mathbf*{\tilde{h}}_i\]&lt;/span&gt; Now, let’s focus on a single node, the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th node. We have to somehow aggregate the node signals from its neighbors. &lt;em&gt;GAT&lt;/em&gt; does so by assigning attention coefficients to each neighbor &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[\alpha_{ij} = \mathrm{softmax}_j (e_{ij}) ~~ e_{ij} = a(\mathbf*{\tilde{h}}_i, \mathbf*{\tilde{h}}_j) = \sigma(\mathbf{a}^{\top} [\mathbf*{\tilde{h}}_i; \mathbf*{\tilde{h}}_j])\]&lt;/span&gt; These coefficients determine how important the signal of node &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is for node &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, and are computed with an attention mechanism called masked attention, implemented as a single layer &lt;em&gt;MLP&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we compute a linear combination of the features of the neighbors, weighted by these attention coefficients. &lt;span class=&#34;math display&#34;&gt;\[\mathbf{h}&amp;#39;_i = \sigma \left( \sum_{j \in \mathcal{N}_i} \alpha_{ij} \mathbf*{\tilde{h}}_j \right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A self-loop is injected in the network since of course the feature of node &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; itself should be taken into consideration, and then a nonlinearity is applied to produce the new hidden feature for node &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/gat_fig.png&#34; alt=&#34;image&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;image&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;We evaluated our model with both the convolutional layers presented before, and also with both the &lt;em&gt;real&lt;/em&gt; and &lt;em&gt;synthetic&lt;/em&gt; data, to draft a comparison. The model performance is evaluated in terms of &lt;em&gt;F1 score&lt;/em&gt;, since we trained it with &lt;em&gt;binary cross entropy&lt;/em&gt; and hence it performs classification. Nevertheless, if we aggregate this prediction, i.e. we employ the graph sum pooling just at inference time, all the models showed better performance on the virality prediction as defined in principle, that is a regression on the whole social graph, with different node signals for different cascades.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Real&lt;/th&gt;
&lt;th&gt;Synthetic&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;GCN&lt;/td&gt;
&lt;td&gt;0.727&lt;/td&gt;
&lt;td&gt;0.744&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;GAT&lt;/td&gt;
&lt;td&gt;0.784&lt;/td&gt;
&lt;td&gt;0.829&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;To recap, in this project we propose a &lt;em&gt;Geometric Deep Learning&lt;/em&gt; approach to the problem of virality prediction on social networks, specifically &lt;em&gt;Twitter&lt;/em&gt;. The main difficulty we faced during the project has been on data. In fact, data was difficult to obtain, we expected to find some datasets that suited our needs, but instead with the &lt;em&gt;GDPR&lt;/em&gt; policies Twitter strictly limited the circulation of its data, and so we had to access it through its &lt;em&gt;APIs&lt;/em&gt; and actually build our own dataset. This leads to the second problem. This data is &lt;strong&gt;sparse&lt;/strong&gt;, in fact very sparse. We think that on social networks information has a natural tendency to spread widely, but this is a &lt;em&gt;bias&lt;/em&gt;, since most of the examples we come up with pop to our mind exactly because they spread widely. We do not think of the majority of content, that simply gets uploaded and shared by little to nobody. So wide spread of information is rare, and this means that a learning model has to learn spreading patterns with very few informative samples.&lt;/p&gt;
&lt;p&gt;This is a general, unsolved problem. We saw some recent related work, solving (so to speak) the problem by carefully selecting informative samples among a huge collection of scraped data. This induces a bias, since the data that the model is shown does not correspond to how data in the real world is distributed. So, a possibility for future work on the project, and in general on this field, might be on how to apply signal processing techniques for reconstructing sparse signals, like &lt;em&gt;compressed sensing&lt;/em&gt;, on &lt;em&gt;non-Euclidean domains&lt;/em&gt;, like graphs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Different approaches for large-scale recommender systems</title>
      <link>//localhost:1313/project/rs/</link>
      <pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/rs/</guid>
      <description>&lt;hr /&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;During the last few decades, we have witnessed the rise of web services offering any kind of goods, take for example Netflix for movies or Amazon for products. Such sites usually have huge catalogues of items that can overwhelm the user with too much information, making it hard for him to find items he would like; being able to narrow this large amount of contents is critical for these services, as it helps them generate greater incomes by suggesting users the right content to buy while also making them stand out from competitors as users find the service more useful.&lt;/p&gt;
&lt;h2 id=&#34;formalization&#34;&gt;Formalization&lt;/h2&gt;
&lt;p&gt;In a very general way, recommender systems are algorithms aimed at suggesting relevant items to users. Various approaches have been tried, among these we can define three families:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;content-based&lt;/em&gt; recommender systems, which create items and users profiles, embed them in a numerical feature space, and then suggest to the user the items which are nearest to him;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;collaborative-filtering&lt;/em&gt; recommender systems, which instead ignore content and rely only on user-item ratings; these furtherly divide in the way they suggest items to the user:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;user-based&lt;/em&gt; approaches suggest him items liked by users similar to him;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;item-based&lt;/em&gt; approaches instead suggest him items similar to items he liked;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;and finally &lt;em&gt;hybrid&lt;/em&gt; recommender systems, which leverage both approaches.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;sequential&#34;&gt;Sequential&lt;/h5&gt;
&lt;p&gt;Recently, recommender systems have taken in consideration sequential dynamics, seeking to capture patterns in the sequence of actions users perform. Contrarily to temporal recommendation systems which explicitly take into account the time of the actions, sequential ones only consider the order of actions, modelling sequential patterns which are independent of time.&lt;/p&gt;
&lt;p&gt;As any sequence pattern recognition task, the problem is challenging, since the number of possible sequences grow exponentially with the number of past actions used as context. Markov-Chain models overcome this issue by conditioning the next action only on the previous few actions, characterizing effectively short-range item transitions. To capture longer-range item dependencies, neural architectures have been used.&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;In this project we will see how recommender systems can be leveraged to suggest &lt;em&gt;games&lt;/em&gt;; to do this, we will use a large dataset of Steam reviews, which is publicly available online.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/steam_wallpaper.jpg&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;The dataset contains information regarding both games and user-game reviews, separated in two tables &lt;em&gt;steam_reviews&lt;/em&gt; and &lt;em&gt;steam_games&lt;/em&gt;; here we can see the schemas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;steam_reviews&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;username&lt;/strong&gt;: reviewer username;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;user_id&lt;/strong&gt;: reviewer id;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;product_id&lt;/strong&gt;: reviewed game id;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;text&lt;/strong&gt;: content of the review;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;date&lt;/strong&gt;: date of the review;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;found_funny&lt;/em&gt;: number of users who found the review funny;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;hours&lt;/em&gt;: number of hours the user played the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;page&lt;/em&gt;: the page in which the review appears;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;page_order&lt;/em&gt;: ??&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;products&lt;/em&gt;: ???&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;compensation&lt;/em&gt;: ??&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;steam_games&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;title&lt;/em&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;id&lt;/em&gt;: game id;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;developer&lt;/em&gt;: company that developed the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;genres&lt;/em&gt;: genres of the game, &lt;em&gt;e.g.&lt;/em&gt; action, adventure and so on;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;metascore&lt;/em&gt;: overall user score of the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;price&lt;/em&gt;: price of the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;discount_price&lt;/em&gt;: discounted price;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;publisher&lt;/em&gt;: company that published the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;release_date&lt;/em&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;reviews_url&lt;/em&gt;: url to the reviews of the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;specs&lt;/em&gt;: characteristics of the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;tags&lt;/em&gt;: tags of the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;url&lt;/em&gt;: link to the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;app_name&lt;/em&gt;: name of the application corresponding to the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;sentiment&lt;/em&gt;: overall sentiment of the game;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;early_access&lt;/em&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will use two models, plus a naive baseline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;MF model&lt;/em&gt;, which is a collaborative-filtering approach;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;RNN-based&lt;/em&gt;, which is a sequential recommendation approach;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both don’t require item profiles, so we will not exploit the rich game features in &lt;em&gt;steam_games&lt;/em&gt;, which may be used in a content-based or hybrid approach.&lt;/p&gt;
&lt;p&gt;Among the listed features in the review table, only the first 5 are needed for our approach, so after a bit of preprocessing we obtain the following features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;user_id&lt;/strong&gt;: reviewer id;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;product_id&lt;/strong&gt;: reviewed game id;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;text&lt;/strong&gt;: content of the review;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;date&lt;/strong&gt;: date of the review;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;so we have all we need to design the models.&lt;/p&gt;
&lt;h2 id=&#34;plots&#34;&gt;Plots&lt;/h2&gt;
&lt;p&gt;The dataset exhibits some non-trivial properties, it is for example evident that the distribution of ratings follows a power-law both concerning users that games:&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/ratings_per_user.png&#34; id=&#34;fig:ratings_user&#34; alt=&#34;Ratings per user.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Ratings per user.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;as we can see in the figure most of the users reviewed few games, while only few users have reviewed a significant number of games. The same thing happens for games, few games received a large amount of reviews, while most games have few.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/ratings_per_game.png&#34; id=&#34;fig:ratings_game&#34; alt=&#34;Ratings per game.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Ratings per game.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The distribution shows a long tail which amounts for a significant part of the catalogue, so a good recommender system should be able to recommend less famous games, even if it is in fact harder.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/long_tail.png&#34; id=&#34;fig:long_tail&#34; alt=&#34;Long tail.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Long tail.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;Two of the models we will see require an explicit ratings matrix in input; nevertheless, our dataset only provides us text reviews, which must be numerically handled somehow. There are approaches which work directly on implicit feedbacks, but considering reviews as binary features which model whether the user-item interaction is present or absent doesn’t make good use of the information, as a review is much richer and can be seen as a verbose rating.&lt;/p&gt;
&lt;p&gt;For this task, we will use a pretrained &lt;em&gt;sentiment analyzer&lt;/em&gt; from StanfordNLP, so every text review will be mapped to a value &lt;span class=&#34;math inline&#34;&gt;\(r \in \{0, 1, 2\}\)&lt;/span&gt;, encoding the sentiment as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;: negative;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;: neutral;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;: positive.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Allowing us to reduce the problem to the classical explicit feedback setting.&lt;/p&gt;
&lt;p&gt;As we can see in the figure, reviews are fairly balanced, with most being neutral. Note that this may be true in the real distribution or a bias coming from the sentiment analyzer.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/sentiments.png&#34; id=&#34;fig:sentiments&#34; alt=&#34;Sentiment distribution.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Sentiment distribution.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;models&#34;&gt;Models&lt;/h1&gt;
&lt;h2 id=&#34;popularity-based&#34;&gt;Popularity based&lt;/h2&gt;
&lt;p&gt;We will consider as a baseline a constant model, that is a model that suggests the same games to every user, independently from his tastes. As we don’t use any information regarding the user, the safest bet is to just suggest the most popular games.&lt;/p&gt;
&lt;h2 id=&#34;matrix-factorization&#34;&gt;Matrix Factorization&lt;/h2&gt;
&lt;p&gt;The first real model that we will try employs a &lt;em&gt;Latent Factor CF&lt;/em&gt; approach. In general, latent factor models are statistical models that relate a set of observable variables (so-called manifest variables) to a set of latent variables. In our case we want to predict user ratings by representing both items and users with a number of hidden factors inferred from observed ratings.&lt;/p&gt;
&lt;p&gt;The basic assumption is that there exist an unknown low-dimensional representation of users and items where user-item affinity can be modeled accurately. &lt;em&gt;Matrix Factorization&lt;/em&gt; is a way to obtain these lower-dimensional representations directly from the observed data.&lt;/p&gt;
&lt;p&gt;In general, we want to infer the rating &lt;span class=&#34;math inline&#34;&gt;\(r_{u, i}\)&lt;/span&gt; of user &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; to item &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;The framework works as follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;map both items and users to a &lt;em&gt;joint latent factor&lt;/em&gt; d-dimensional space, so&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;each user will be represented by &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_u \in \mathbb{R}^d\)&lt;/span&gt;,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;each item will be represented by &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}_i \in \mathbb{R}^d\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;estimate &lt;span class=&#34;math inline&#34;&gt;\(r_{u, i}\)&lt;/span&gt; by applying the dot product &lt;span class=&#34;math display&#34;&gt;\[\hat{r}_{u, i} = \mathbf{x}^T_u \cdot \mathbf{w}_i = \sum_{j=1}^{d} x_{u, j} w_{j, i}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;recommend to user &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; items for which the estimate is maximum.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Easier said than done, as we need a reasonable way to map users and items to these latent factor vectors.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/MF.png&#34; id=&#34;fig:MF&#34; alt=&#34;Matrix Factorization.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Matrix Factorization.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;What we can do is leverage the observed ratings which are contained in the matrix &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;, and try to approximate &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; with the product of two matrices &lt;span class=&#34;math inline&#34;&gt;\(X \in \mathbb{R}^{m \times d}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(W \in \mathbb{R}^{d \times n}\)&lt;/span&gt;. This is equivalent to find the parameters that minimize the following loss function &lt;span class=&#34;math display&#34;&gt;\[\mathcal{L}(X, W) = \sum_{u, i \in D} \left( r_{u, i} - \mathbf{x}^T_u \cdot \mathbf{w}_i \right) ^2\]&lt;/span&gt; plus possibly a regularization term.&lt;/p&gt;
&lt;p&gt;The resulting optimization problem can be solved either with SGD or ALS; The former is an iterative method which tries to minimize the loss function by descending its gradient, going opposite to the direction of steepest increase; nevertheless, this approach doesn’t scale well when &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; grows large, therefore given the dimension of the ratings matrix in our scenario this may not be convenient.&lt;/p&gt;
&lt;p&gt;ALS overcomes the non-convexity of the objective function by alternately fixing one latent vector and updating the other one; when one latent vector is fixed, the objective becomes quadratic and thus convex, allowing to find a closed-form solution.&lt;/p&gt;
&lt;h2 id=&#34;sequential-1&#34;&gt;Sequential&lt;/h2&gt;
&lt;h3 id=&#34;formalization-1&#34;&gt;Formalization&lt;/h3&gt;
&lt;p&gt;To use sequential techniques we need to properly transform the dataset, obtaining for each user a vector &lt;span class=&#34;math display&#34;&gt;\[\mathbf{x} = (x_1, \dots, x_n)\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the embedding of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th game reviewed by the user, ordered by timestamp. We want the model to be able to predict the next game that will be reviewed by the user given the previous reviewed games, that is predicting &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_{i-1}\)&lt;/span&gt;; To do this, we give the model the sequence of the first &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; reviews &lt;span class=&#34;math inline&#34;&gt;\((x_1, \dots, x_{n-1})\)&lt;/span&gt; as input and train it to predict the input shifted by 1 position &lt;span class=&#34;math inline&#34;&gt;\((x_2, \dots, x_{n})\)&lt;/span&gt;. This is basically what in NLP is called &lt;em&gt;language modeling&lt;/em&gt;, with the game ids composing the vocabulary.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/input_preprocess.png&#34; id=&#34;fig:input-preprocess&#34; alt=&#34;Input preprocessing.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Input preprocessing.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Cross entropy is used as loss function, as each token prediction is a multilabel classification task &lt;span class=&#34;math display&#34;&gt;\[\mathcal{L} = -\sum_{c=1}^My_{o,c}\log(p_{o,c})\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;lstm&#34;&gt;LSTM&lt;/h3&gt;
&lt;p&gt;As we have previously introduced, sequential recommender systems can exploit specialized neural architectures such as &lt;em&gt;Recurrent Neural Networks&lt;/em&gt;; these summarize the context of a certain token &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; with a recurrently updated vector &lt;span class=&#34;math display&#34;&gt;\[\vec{h}_m = g(\vec{x}_m, \vec{h}_{m-1}), \quad m = 1,2, \dots, m\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}_m\)&lt;/span&gt; is the vector embedding of the token &lt;span class=&#34;math inline&#34;&gt;\(w_m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; defines the recurrence. Nevertheless, RNNs often fail to capture long-time dependencies; for this reason, LSTMs are often used. These employ a more complex recurrence, in which a memory cell goes through a series of gates, in fact avoiding repeated applications of non-linearity. The hidden state &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_m\)&lt;/span&gt; accounts for information in the input leading up to position &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;, but it ignores the subsequent tokens, which may also be relevant to the tag &lt;span class=&#34;math inline&#34;&gt;\(y_m\)&lt;/span&gt;; this can be addressed by adding a second LSTM, in which the input is reversed. This architecture is called &lt;em&gt;Bidirectional LSTM&lt;/em&gt;, and is one of the most effective neural architectures for sequences; nevertheless, for how we modeled the problem it would allow the model to cheat, as for any intermediate prediction it would be able to peek at the next game in the sequence and give it in output correctly.&lt;/p&gt;
&lt;p&gt;Since the LSTM expects a batch of sequences of equal length, &lt;em&gt;padding&lt;/em&gt; is added.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/neural_architecture.png&#34; id=&#34;fig:model-arch&#34; alt=&#34;Model architecture.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Model architecture.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The model thus works as follows:&lt;/p&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;&lt;p&gt;each input sequence &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; is embedded by a word embedding layer;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it is then passed to a &lt;em&gt;LSTM&lt;/em&gt; encoder which takes as input the embedded sequence and returns a dynamic representation of each game and its context;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the hidden representation is then given to a &lt;em&gt;Multi Layer Perceptron&lt;/em&gt; that maps each game representation to the games space.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Root Mean Squared Error&lt;/em&gt; has been used as metric to evaluate the Matrix Factorization model, &lt;span class=&#34;math display&#34;&gt;\[{\displaystyle \text{RMSD} ={\sqrt {\frac {\sum _{t=1}^{T}({\hat {y}}_{t}-y_{t})^{2}}{T}}}.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;while &lt;em&gt;Hit@&lt;/em&gt;&lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; has been used for the sequential model, which is a &lt;em&gt;top&lt;/em&gt;-&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; metric that counts the fraction of times that the ground-truth next item is among the top &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; recommended items.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\text{# hits}}{\text{# users}}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;results-1&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;matrix-factorization-1&#34;&gt;Matrix Factorization&lt;/h3&gt;
&lt;p&gt;In the table we can see the &lt;em&gt;RMSE&lt;/em&gt; for the MF model against the popularity based baseline; while the results may not seem impressive, it must be noted that the problem is in fact challenging: sentiment analysis is still an open research field, the pretrained model that we have seen is said to reach an accuracy of &lt;span class=&#34;math inline&#34;&gt;\(70\%\)&lt;/span&gt;, the resulting error pile-up with the error produced by the recommender system may be severe; moreover, the game reviews may be particularly difficult for the analyzer to get right, as gamers are often ironic and have their own niche vocabulary of words and meanings.&lt;/p&gt;
&lt;h3 id=&#34;sequential-2&#34;&gt;Sequential&lt;/h3&gt;
&lt;p&gt;In the table we can see the &lt;em&gt;hit@&lt;/em&gt;10 score for the sequential model against the popularity baseline; an accuracy of &lt;span class=&#34;math inline&#34;&gt;\(0.503\)&lt;/span&gt; would be low in a typical classification setting with few classes, especially if we consider that the model has 10 tries; nevertheless, in this scenario the model must be able to discriminate among &lt;span class=&#34;math inline&#34;&gt;\(8590\)&lt;/span&gt; classes; to understand what that means, let’s see what score would achieve a random baseline. &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
    P\left\{hit\right\} &amp;amp;= \sum_{i = 1}^{10} P\left\{pred_i \text{ is correct} \right\}\\
    &amp;amp;= \sum_{i=1}^{10} \frac{1}{\text{# classes}} = 10 \cdot \frac{1}{8590} \approx 1e^{-4}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;study-case&#34;&gt;Study case&lt;/h2&gt;
&lt;p&gt;As a study case, I tried to feed the model with my own rated games to see if the predictions fit my tastes; Let’s try to evaluate just the recommending part, so I am going to give the games directly numeric ratings so to skip the sentiment analyzer part.&lt;/p&gt;
&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;We have seen two very different approaches to recommender systems, neither of them reached state of the art: for example, attention-based sequential recommender systems have reached an hit ratio at &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(0.7\)&lt;/span&gt; over the same dataset. The MF approach we have seen is too naive and doesn’t exploit all the available features which would make up for a good hybrid based recommender system, while the sequential approach using RNNs has few intrinsic flaws, for example the LSTM outputs a sequence of games where the same game is often repeated while it can’t happen in the gold truth, nevertheless it is not easy to carve such constraints in the model.&lt;/p&gt;
&lt;p&gt;It would certainly be interesting to add content based features to both approaches, for example obtaining contextualized embeddings from the text reviews. For the sequential setting, it may be worth trying adding a sequence scorer on top of the LSTM, like a &lt;em&gt;Conditional Random Field&lt;/em&gt;, to help assess the quality of a sequence of tags as a whole.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>//localhost:1313/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>//localhost:1313/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>//localhost:1313/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to Wowchemy, the website builder for Hugo</title>
      <link>//localhost:1313/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/post/getting-started/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;











&lt;figure  id=&#34;figure-the-template-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;
  &lt;div class=&#34;figure-img-wrap&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/master/academic.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;&lt;figcaption&gt;
      The template is mobile first with a responsive design to ensure that your site looks stunning on every device.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;👉 &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📚 &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💬 &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🐦 Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%28%23MadeWithWowchemy%20OR%20%23MadeWithAcademic%29&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💡 &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomplans&#34;&gt;&lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;❤️ Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ❤️&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features 🦄✨&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-admin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy Admin&lt;/a&gt;:&lt;/strong&gt; An admin tool to automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>//localhost:1313/publication/example/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/publication/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
