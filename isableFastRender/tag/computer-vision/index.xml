<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision | Donato Crisostomi</title>
    <link>//localhost:1313/tag/computer-vision/</link>
      <atom:link href="//localhost:1313/tag/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 15 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>//localhost:1313/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Computer Vision</title>
      <link>//localhost:1313/tag/computer-vision/</link>
    </image>
    
    <item>
      <title>Towards Conditionality for Probabilistic Diffusion Models</title>
      <link>//localhost:1313/project/is/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/is/</guid>
      <description>&lt;!--   We attempt to generate class-conditional images using a probabilistic
  diffusion model by adapting to the latter class-conditional techniques
  initially developed for *GANs*. In particular, we experiment two
  architectures, one leveraging *Conditional Batch Norm* and one
  integrating an *Auxiliary Classifier*, testing different resolutions
  and configurations. The results suggest that these approaches cannot
  be applied to diffusion models as they are: the *CBN* architecture
  results in class-conditional images that do not look realistic while
  the *AC* one yields prettier images but fails to capture the
  conditionality. --&gt;
&lt;hr /&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Richard Feynman once said that&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;What I cannot create, I do not understand.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;and, in the context of machine learning, this means that for machines to understand their input data, they should learn to create it. Moreover, being able to generate unseen images opens the door to some groundshaking applications, from super-resolution, to text-to-image translation (Ledig et al. 2016; Gorti and Ma 2018).&lt;/p&gt;
&lt;p&gt;Image synthesis nevertheless has been one of the most challenging tasks for machine learning to tackle. In fact, generative models are needed to generate new unseen images; but while we witnessed a huge leap forward in discriminative models during the first years of the last decade thanks to neural architectures, generative models initially failed to keep up. It was in &lt;span class=&#34;math inline&#34;&gt;\(2014\)&lt;/span&gt; that &lt;em&gt;Goodfellow et al.&lt;/em&gt; came up with &lt;em&gt;Generative Adversarial Networks&lt;/em&gt; (Goodfellow et al. 2014). GANs and their evolutions have been the state-of-the-art since then, but a very recent paper shows that similar performance can be obtained with a different model that leverages probabilistic diffusion in order to generate images (Ho, Jain, and Abbeel 2020).&lt;/p&gt;
&lt;p&gt;Generative models have also been particularly useful to create artificial examples in order to augment datasets (Santos Tanaka and Aranha 2019), but in order to generate new images belonging to a certain class, one would need to have a conditional model. Goal of this research is therefore to integrate class-conditionality in probabilistic diffusion models.&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related work&lt;/h1&gt;
&lt;p&gt;As anticipated, the most used generative architecture is &lt;em&gt;Generative Adversarial Networks&lt;/em&gt; (Goodfellow et al. 2014), which is composed of two networks that are trained &lt;em&gt;adversarily&lt;/em&gt;: a &lt;em&gt;generator&lt;/em&gt; is trained in such a way that a &lt;em&gt;discriminator&lt;/em&gt; cannot distinguish between its generated samples and the real ones, while simultaneously training the discriminator to be able to distinguish between fake samples and real ones. To integrate class-conditionality in GANs, various approaches have been tried: &lt;em&gt;Brock et al.&lt;/em&gt; provide class information to the generator with &lt;em&gt;conditional batch norm&lt;/em&gt; (Brock, Donahue, and Simonyan 2018), while &lt;em&gt;Odena et al.&lt;/em&gt; leverage an auxiliary classifier (Odena, Olah, and Shlens 2017).&lt;/p&gt;
&lt;h1 id=&#34;proposed-method&#34;&gt;Proposed method&lt;/h1&gt;
&lt;p&gt;A &lt;em&gt;diffusion probabilistic model&lt;/em&gt; is a parameterized &lt;em&gt;Markov chain&lt;/em&gt;: A Markov chain models the state of a system with a random variable that changes through time. For the Markov property to hold, the distribution of a state must depend only on the distribution of the previous state.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/markov-diffusion.png&#34; id=&#34;fig:markov&#34; alt=&#34;The directed graphical model used in the project.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;The directed graphical model used in the project.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The training phase consists of two phases: a forward pass and a reverse pass, as can be seen in &lt;a href=&#34;#fig:markov&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:markov&#34;&gt;1&lt;/a&gt;. In the former, also called the diffusion process, Gaussian noise is added to the image according to a fixed schedule so each transition in the Markov chain &lt;span class=&#34;math inline&#34;&gt;\(q(\mathbf{x}_{t}| \mathbf{x}_{t-1})\)&lt;/span&gt; represents the addition of Gaussian noise. In the latter, the transitions of a reverse Markov chain are learned in order to reconstruct the destroyed signal; the parameters are learned by optimizing the variational bound on negative loglikelihood:&lt;/p&gt;
&lt;div class=&#34;math&#34;&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \mathbb{E}\left[ - \log p_{\theta}(\mathbf{x}_0) \right] &amp;amp;\leq \mathbb{E}_q \left[ - \log \frac{p_\theta (\mathbf{x}_0, \dots, \mathbf{x}_T)}{q(\mathbf{x}_1, \dots, \mathbf{x}_T|\mathbf{x}_0)}\right] \\
  &amp;amp;= \mathbb{E}_q\left[ - \log \ p(\mathbf{x}_T) - \sum_{t \geq 1} \log \frac{p_\theta (\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t | \mathbf{x}_{t-1})} \right]
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We employ the architecture suggested by the original paper, in which the denoiser is a &lt;em&gt;U-Net&lt;/em&gt; (Ronneberger, Fischer, and Brox 2015), shown in &lt;a href=&#34;#fig:u-net&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:u-net&#34;&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/U-net.png&#34; id=&#34;fig:u-net&#34; alt=&#34;The popular U-Net architecture used for the denoiser.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;The popular &lt;em&gt;U-Net&lt;/em&gt; architecture used for the denoiser.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;conditional-batch-norm&#34;&gt;Conditional Batch Norm&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Conditional Batch Normalization&lt;/em&gt; was first applied to language-vision tasks to implement the intuition that the linguistic input should modulate the entire visual processing, instead of being fused only in the last part of the process. &lt;em&gt;CBN&lt;/em&gt; builds upon &lt;em&gt;Batch Normalization&lt;/em&gt;, in which each batch is normalized as follows to reduce the internal co-variate shift &lt;span class=&#34;math display&#34;&gt;\[\text{BN}_{\gamma, \beta}(x_i) = \gamma_i \frac{x_i - \mathbb{E}(x_i)}{\sqrt{var(x_i)}} + \beta_i\]&lt;/span&gt; In &lt;em&gt;CBN&lt;/em&gt; we want to predict &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; from an embedding of the class, so that the class may manipulate entire feature maps by scaling them up or down, negating them, or shutting them off completely (Odena, Olah, and Shlens 2017).&lt;/p&gt;
&lt;p&gt;The integration of &lt;em&gt;CBN&lt;/em&gt; in the architecture is done by replacing the &lt;em&gt;Batch Norm&lt;/em&gt; layers inside the denoiser architecture with &lt;em&gt;conditional&lt;/em&gt; ones. We are going to refer to the model obtained by adding &lt;em&gt;CBN&lt;/em&gt; to the original model as &lt;span class=&#34;math inline&#34;&gt;\(M_{CBN}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;auxiliary-classifier&#34;&gt;Auxiliary Classifier&lt;/h2&gt;
&lt;p&gt;Analogously to what has been done in (Odena, Olah, and Shlens 2017) for GANs, we have added an auxiliary classifier to the original architecture of the denoiser.&lt;/p&gt;
&lt;p&gt;To provide the class information to the denoiser, the label is embedded and reshaped to be the same dimension as one of the channels of the image, &lt;em&gt;i.e.&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(w \times h\)&lt;/span&gt;, and then concatenated to the input image in the channel dimension. Images are thus tensors of shape &lt;span class=&#34;math inline&#34;&gt;\((b, c+1, w, h)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the batch size, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the number of channels (RGB), &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is the width and &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is the height.&lt;/p&gt;
&lt;p&gt;The overall loss is then obtained as a weighted sum of the variational loss to account for the reconstruction error, and the classifier loss, which is a categorical cross entropy, where the weight is a hyper-parameter. The loss should this way be enriched with class information that should backpropagate to the parameters that are involved in the generation.&lt;/p&gt;
&lt;p&gt;We are going to refer to the model obtained by adding the auxiliary classifier to the original model as &lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;
&lt;p&gt;We based our implementation on the following repository &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;denoising-diffusion-pytorch&lt;/a&gt;, which provides a working PyTorch baseline.&lt;/p&gt;
&lt;p&gt;Our original goal was to apply the model to the &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_IP102_A_Large-Scale_Benchmark_Dataset_for_Insect_Pest_Recognition_CVPR_2019_paper.pdf&#34;&gt;insect-pest dataset&lt;/a&gt; to create new artificial samples for dataset augmentation. The original dataset consisted of over &lt;span class=&#34;math inline&#34;&gt;\(75k\)&lt;/span&gt; images, but most of the classes had few samples and low variance between them, we therefore used a subsample of &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; classes, ammounting to &lt;span class=&#34;math inline&#34;&gt;\(\approx 25\)&lt;/span&gt;k samples. The dataset is not really what a data scientist would dream of, as no bounding boxes were provided, and it is often hard even for humans to understand what’s in the image. To attribute the right degree of responsibility to the model and to the dataset, we also tested the model on a different dataset from Stanford, containing &lt;span class=&#34;math inline&#34;&gt;\(\approx 20k\)&lt;/span&gt; images of cars.&lt;/p&gt;
&lt;p&gt;To test our proposed conditional methods, to simplify the visual inspection of the results, we instead created an ad-hoc dataset of only two classes with the aim of maximizing the difference between them. To this end, we took a subset of &lt;span class=&#34;math inline&#34;&gt;\(\approx 10k\)&lt;/span&gt; images from the &lt;em&gt;Stanford dogs&lt;/em&gt; (Khosla et al. 2011) and the &lt;em&gt;Stanford cars&lt;/em&gt; (Krause et al. 2013) datasets.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;All the unconditional and conditional versions of the model that follow have been trained for &lt;span class=&#34;math inline&#34;&gt;\(\approx 100\)&lt;/span&gt; epochs. The unconditional model was tested both on low resolution sample and higher resolution ones, yielding the results that follow.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;64&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;128&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/unconditional-insects-64.png&#34; id=&#34;fig:insects-64&#34; alt=&#34;Unconditional model applied to the insect-pest dataset with resolution 64.&#34; /&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/unconditional-insects-128.png&#34; id=&#34;fig:insects-128&#34; alt=&#34;Unconditional model applied to the insect-pest dataset with resolution 128.&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As it is evident from the samples, the resolution plays a strong role in generating realistic images, providing the model more information to leverage for the generation. The Inception Scores are as follow&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;64&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;128&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;insects&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;4.2&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;4.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;cars&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;3.32&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;Regarding the conditional model, the two proposed methods yielded totally different results. &lt;span class=&#34;math inline&#34;&gt;\(M_{CBN}\)&lt;/span&gt; converges to a small reconstruction error, and the class of the generated images can often be inferred visually; see for example &lt;a href=&#34;#fig:cbn-generated-dogs&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:cbn-generated-dogs&#34;&gt;6&lt;/a&gt; which is a batch of generated images for the ‘dog’ class and &lt;a href=&#34;#fig:cbn-generated-cars&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:cbn-generated-cars&#34;&gt;7&lt;/a&gt; which is a batch of generated images for the ‘car’ class.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;class ‘dog’&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;class ‘car’&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/cbn-generated-dogs.png&#34; id=&#34;fig:cbn-generated-dogs&#34; alt=&#34;Images generated by M_{CBN} when supplied class ‘dog.’&#34; /&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/cbn-generated-cars.png&#34; id=&#34;fig:cbn-generated-cars&#34; alt=&#34;Images generated by M_{CBN} when supplied class ‘car.’&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Nevertheless, the results appear as messy color spots which do not resemble any realistic image. As the reconstruction error is small, the problem seems to be related to the sampling procedure, and indeed it might be the case that the class information is not accounted for correctly during sampling, as the &lt;em&gt;CBN&lt;/em&gt; is only part of the denoiser and class information does not influence the rest of the sampling process.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/CBN-t-sne-points.png&#34; id=&#34;fig:cbn-t-sne-points&#34; alt=&#34;t-sne plot of the images generated by M_{CBN}.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;&lt;em&gt;t-sne&lt;/em&gt; plot of the images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{CBN}\)&lt;/span&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To check whether there is a class-related distinction between the generated images, we plotted the images with &lt;em&gt;t-SNE&lt;/em&gt;, yielding the results that can be seen in &lt;a href=&#34;#fig:cbn-t-sne-points&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:cbn-t-sne-points&#34;&gt;11&lt;/a&gt; and &lt;a href=&#34;#fig:cbn-t-sne-images&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:cbn-t-sne-images&#34;&gt;12&lt;/a&gt;. The points seem to be fairly separable, indicating that the class is indeed infused in the generated images.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/CBN-t-sne-images.png&#34; id=&#34;fig:cbn-t-sne-images&#34; alt=&#34;t-sne plot of the images generated by M_{CBN} with each point visualized as the image that it embeds.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;&lt;em&gt;t-sne&lt;/em&gt; plot of the images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{CBN}\)&lt;/span&gt; with each point visualized as the image that it embeds.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt; instead results in the converse, yielding almost realistic images that do not seem to be much influenced by the class. As a first attempt, we tried training the random-initialized classifier with the rest of the architecture; this resulted in a rapidly decreasing classifier loss that did not help the generation at all, but instead seemed to only worsen the results. To our advise, this was due to a process of co-adaption in which the parameters of one computational block were set to satisfy the other, and viceversa. To address this issue, we pretrained the classifier until convergence on the dataset of real images and then kept its parameters fixed during the training of the rest of the model. This yielded the results in &lt;a href=&#34;#fig:ac-generated-dogs&#34;&gt;8&lt;/a&gt; and &lt;a href=&#34;#fig:ac-generated-cars&#34;&gt;9&lt;/a&gt;;&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;col style=&#34;width: 50%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;class ‘dog’&lt;/th&gt;
&lt;th style=&#34;text-align: center;&#34;&gt;class ‘car’&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/ac-generated-dogs.png&#34; id=&#34;fig:ac-generated-dogs&#34; alt=&#34;Images generated by M_{AC} when supplied class ‘dog.’&#34; /&gt;&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;figures/ac-generated-cars.png&#34; id=&#34;fig:ac-generated-cars&#34; alt=&#34;Images generated by M_{AC} when supplied class ‘car.’&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The images resemble cars, mostly ignoring the input label. We eventually tried feeding higher-resolution images to the model, but with no significant improvement. The generated images in fact do not resemble their class, but the classification loss still goes rapidly down; to provide an explanation, we visually inspected the images and found out that artifacts were present in every image (&lt;a href=&#34;#fig:ac-generated-128&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:ac-generated-128&#34;&gt;10&lt;/a&gt;), probably resulting from the generator ‘tricking’ the classifier, emphasizing features that resulted in high confidence guesses in the latter.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/ac-generated-128.png&#34; id=&#34;fig:ac-generated-128&#34; alt=&#34;Images generated by M_{AC} when supplied class ‘car’ at resolution 128; artifacts are circled.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;Images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt; when supplied class ‘car’ at resolution &lt;span class=&#34;math inline&#34;&gt;\(128\)&lt;/span&gt;; artifacts are circled.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We concluded that the model was not robust enough, and therefore tried to employ a finetuned &lt;em&gt;ResNet18&lt;/em&gt; classifier, but this did not solve the issue.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/AC-t-sne-points.png&#34; id=&#34;fig:ac-t-sne-points&#34; alt=&#34;t-sne plot of the images generated by M_{AC}.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;&lt;em&gt;t-sne&lt;/em&gt; plot of the images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As before, we plotted the images with &lt;em&gt;t-SNE&lt;/em&gt; to check whether there is a class-related distinction between the images; as can be seen in &lt;a href=&#34;#fig:ac-t-sne-points&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:ac-t-sne-points&#34;&gt;13&lt;/a&gt; and &lt;a href=&#34;#fig:ac-t-sne-images&#34; data-reference-type=&#34;ref&#34; data-reference=&#34;fig:ac-t-sne-images&#34;&gt;14&lt;/a&gt;, this time the points are all mixed up, indicating that the model fails to conditionate the generation on the class.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;figures/AC-t-sne-images.png&#34; id=&#34;fig:ac-t-sne-images&#34; alt=&#34;t-sne plot of the images generated by M_{AC} with each point visualized as the image that it embeds.&#34; /&gt;&lt;figcaption aria-hidden=&#34;true&#34;&gt;&lt;em&gt;t-sne&lt;/em&gt; plot of the images generated by &lt;span class=&#34;math inline&#34;&gt;\(M_{AC}\)&lt;/span&gt; with each point visualized as the image that it embeds.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;The proposed methods do not yield acceptable results, indicating that it is not enough to adapt GANs techniques for class-conditionality to probabilistic diffusion models, while this is also not straightforward to do. This also emphasizes that, while seemingly close to GANs, this family of models requires ad-hoc research, as they are based on different theorical aspects.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; role=&#34;doc-bibliography&#34;&gt;
&lt;div id=&#34;ref-Brock2018gan&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Brock, Andrew, Jeff Donahue, and Karen Simonyan. 2018. “Large Scale GAN Training for High Fidelity Natural Image Synthesis.” &lt;em&gt;CoRR&lt;/em&gt; abs/1809.11096. &lt;a href=&#34;http://arxiv.org/abs/1809.11096&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1809.11096&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-goodfellow2014generative&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Networks.” &lt;a href=&#34;http://arxiv.org/abs/1406.2661&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1406.2661&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-text2image&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Gorti, Satya Krishna, and Jeremy Ma. 2018. “Text-to-Image-to-Text Translation Using Cycle Consistent Adversarial Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1808.04538. &lt;a href=&#34;http://arxiv.org/abs/1808.04538&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1808.04538&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-ho2020denoising&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. “Denoising Diffusion Probabilistic Models.” &lt;a href=&#34;http://arxiv.org/abs/2006.11239&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/2006.11239&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-stanford-dogs&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Khosla, Aditya, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. 2011. “Novel Dataset for Fine-Grained Image Categorization.” In &lt;em&gt;First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;. Colorado Springs, CO.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-stanford-cars&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Krause, Jonathan, Michael Stark, Jia Deng, and Li Fei-Fei. 2013. “3d Object Representations for Fine-Grained Categorization.” In &lt;em&gt;4th International IEEE Workshop on 3d Representation and Recognition (3dRR-13)&lt;/em&gt;. Sydney, Australia.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-super-resolution&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Ledig, Christian, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew P. Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. 2016. “Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.” &lt;em&gt;CoRR&lt;/em&gt; abs/1609.04802. &lt;a href=&#34;http://arxiv.org/abs/1609.04802&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1609.04802&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-odena2017conditional&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Odena, Augustus, Christopher Olah, and Jonathon Shlens. 2017. “Conditional Image Synthesis with Auxiliary Classifier GANs.” &lt;a href=&#34;http://arxiv.org/abs/1610.09585&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1610.09585&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-U-net&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” &lt;em&gt;CoRR&lt;/em&gt; abs/1505.04597. &lt;a href=&#34;http://arxiv.org/abs/1505.04597&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1505.04597&lt;/a&gt;.
&lt;/div&gt;
&lt;hr /&gt;
&lt;div id=&#34;ref-data-augmentation&#34; class=&#34;csl-entry&#34; role=&#34;doc-biblioentry&#34;&gt;
Santos Tanaka, Fabio Henrique Kiyoiti dos, and Claus Aranha. 2019. “Data Augmentation Using GANs.” &lt;em&gt;CoRR&lt;/em&gt; abs/1904.09135. &lt;a href=&#34;http://arxiv.org/abs/1904.09135&#34; class=&#34;uri&#34;&gt;http://arxiv.org/abs/1904.09135&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
